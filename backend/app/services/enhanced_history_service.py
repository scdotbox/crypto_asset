"""
å¢å¼ºçš„å†å²æ•°æ®æœåŠ¡

æä¾›æ›´é«˜é¢‘çš„æ•°æ®é‡‡é›†å’Œæ›´ä¸°å¯Œçš„è¶‹åŠ¿åˆ†æåŠŸèƒ½ï¼Œ
å€Ÿé‰´CoinBankç­‰ä¸“ä¸šå¹³å°çš„æ•°æ®å¤„ç†æ–¹å¼ã€‚
"""

import asyncio
import json
import time
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass

from app.core.logger import get_logger
from app.core.config import settings
from app.core.database import db_manager
from app.services.asset_service import AssetService
from app.services.price_service import PriceService
from app.services.history_cache_service import HistoryCacheService

logger = get_logger(__name__)


@dataclass
class TrendPoint:
    """è¶‹åŠ¿æ•°æ®ç‚¹"""
    timestamp: int
    date: str
    total_value: float
    asset_count: int
    top_assets: List[Dict[str, Any]]
    price_changes: Dict[str, float]  # 24hä»·æ ¼å˜åŒ–
    portfolio_distribution: Dict[str, float]  # èµ„äº§åˆ†å¸ƒ


@dataclass
class PortfolioMetrics:
    """æŠ•èµ„ç»„åˆæŒ‡æ ‡"""
    total_value: float
    total_change_24h: float
    total_change_7d: float
    total_change_30d: float
    asset_count: int
    top_performer: Optional[Dict[str, Any]]
    worst_performer: Optional[Dict[str, Any]]
    diversity_score: float  # å¤šæ ·åŒ–è¯„åˆ†


class EnhancedHistoryService:
    """å¢å¼ºçš„å†å²æ•°æ®æœåŠ¡"""
    
    def __init__(self):
        self.asset_service = AssetService()
        self.price_service = PriceService()
        self.history_cache = HistoryCacheService()
        
        # æ•°æ®é‡‡é›†é…ç½®
        self.collection_intervals = {
            'real_time': 5 * 60,      # 5åˆ†é’Ÿ
            'hourly': 60 * 60,        # 1å°æ—¶
            'daily': 24 * 60 * 60,    # 1å¤©
        }
        
        # è¿è¡ŒçŠ¶æ€
        self.is_collecting = False
        self.collection_tasks = {}

    async def start_enhanced_collection(self):
        """å¯åŠ¨å¢å¼ºçš„æ•°æ®é‡‡é›†"""
        if self.is_collecting:
            logger.warning("æ•°æ®é‡‡é›†å·²åœ¨è¿è¡Œä¸­")
            return
        
        self.is_collecting = True
        logger.info("ğŸš€ å¯åŠ¨å¢å¼ºå†å²æ•°æ®é‡‡é›†æœåŠ¡")
        
        # å¯åŠ¨ä¸åŒé¢‘ç‡çš„é‡‡é›†ä»»åŠ¡
        self.collection_tasks = {
            'hourly': asyncio.create_task(self._hourly_collection()),
            'daily': asyncio.create_task(self._daily_collection()),
        }
        
        # å¦‚æœå¯ç”¨å®æ—¶é‡‡é›†
        if getattr(settings, 'enable_realtime_collection', False):
            self.collection_tasks['real_time'] = asyncio.create_task(
                self._realtime_collection()
            )

    async def stop_enhanced_collection(self):
        """åœæ­¢å¢å¼ºçš„æ•°æ®é‡‡é›†"""
        self.is_collecting = False
        logger.info("ğŸ›‘ åœæ­¢å¢å¼ºå†å²æ•°æ®é‡‡é›†æœåŠ¡")
        
        # å–æ¶ˆæ‰€æœ‰é‡‡é›†ä»»åŠ¡
        for task_name, task in self.collection_tasks.items():
            if not task.done():
                task.cancel()
                try:
                    await task
                except asyncio.CancelledError:
                    logger.info(f"å·²å–æ¶ˆ {task_name} é‡‡é›†ä»»åŠ¡")
        
        self.collection_tasks.clear()

    async def _realtime_collection(self):
        """å®æ—¶æ•°æ®é‡‡é›†ï¼ˆ5åˆ†é’Ÿé—´éš”ï¼‰"""
        while self.is_collecting:
            try:
                await self._collect_portfolio_snapshot('real_time')
                await asyncio.sleep(self.collection_intervals['real_time'])
            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error(f"å®æ—¶æ•°æ®é‡‡é›†å¤±è´¥: {e}")
                await asyncio.sleep(60)  # å‡ºé”™åç­‰å¾…1åˆ†é’Ÿ

    async def _hourly_collection(self):
        """å°æ—¶çº§æ•°æ®é‡‡é›†"""
        while self.is_collecting:
            try:
                await self._collect_portfolio_snapshot('hourly')
                await asyncio.sleep(self.collection_intervals['hourly'])
            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error(f"å°æ—¶çº§æ•°æ®é‡‡é›†å¤±è´¥: {e}")
                await asyncio.sleep(300)  # å‡ºé”™åç­‰å¾…5åˆ†é’Ÿ

    async def _daily_collection(self):
        """æ—¥çº§æ•°æ®é‡‡é›†å’Œåˆ†æ"""
        while self.is_collecting:
            try:
                await self._collect_portfolio_snapshot('daily')
                await self._generate_daily_analytics()
                await asyncio.sleep(self.collection_intervals['daily'])
            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error(f"æ—¥çº§æ•°æ®é‡‡é›†å¤±è´¥: {e}")
                await asyncio.sleep(3600)  # å‡ºé”™åç­‰å¾…1å°æ—¶

    async def _collect_portfolio_snapshot(self, collection_type: str):
        """é‡‡é›†æŠ•èµ„ç»„åˆå¿«ç…§"""
        try:
            logger.debug(f"ğŸ“¸ å¼€å§‹ {collection_type} æŠ•èµ„ç»„åˆå¿«ç…§é‡‡é›†")
            
            # è·å–å½“å‰æ‰€æœ‰èµ„äº§
            assets = await self.asset_service.get_detailed_assets()
            if not assets:
                logger.debug("æ²¡æœ‰èµ„äº§éœ€è¦é‡‡é›†")
                return
            
            timestamp = int(time.time())
            date = datetime.fromtimestamp(timestamp).isoformat()
            
            # è®¡ç®—æŠ•èµ„ç»„åˆæŒ‡æ ‡
            metrics = await self._calculate_portfolio_metrics(assets)
            
            # ä¿å­˜å¿«ç…§æ•°æ®
            await self._save_enhanced_snapshot(
                timestamp, date, assets, metrics, collection_type
            )
            
            logger.debug(f"âœ… {collection_type} å¿«ç…§é‡‡é›†å®Œæˆï¼Œæ€»ä»·å€¼: ${metrics.total_value:.2f}")
            
        except Exception as e:
            logger.error(f"æŠ•èµ„ç»„åˆå¿«ç…§é‡‡é›†å¤±è´¥: {e}")

    async def _calculate_portfolio_metrics(self, assets: List[Any]) -> PortfolioMetrics:
        """è®¡ç®—æŠ•èµ„ç»„åˆæŒ‡æ ‡"""
        total_value = 0.0
        asset_values = []
        
        for asset in assets:
            value = getattr(asset, 'value_usdc', 0) or 0
            total_value += value
            asset_values.append({
                'symbol': asset.token_symbol,
                'value': value,
                'quantity': getattr(asset, 'quantity', 0) or 0,
                'price': getattr(asset, 'price_usdc', 0) or 0,
            })
        
        # æ’åºæ‰¾å‡ºè¡¨ç°æœ€å¥½å’Œæœ€å·®çš„èµ„äº§
        asset_values.sort(key=lambda x: x['value'], reverse=True)
        top_performer = asset_values[0] if asset_values else None
        worst_performer = asset_values[-1] if len(asset_values) > 1 else None
        
        # è®¡ç®—å¤šæ ·åŒ–è¯„åˆ†ï¼ˆåŸºäºèµ„äº§åˆ†å¸ƒçš„å‡åŒ€ç¨‹åº¦ï¼‰
        diversity_score = self._calculate_diversity_score(asset_values, total_value)
        
        return PortfolioMetrics(
            total_value=total_value,
            total_change_24h=0.0,  # TODO: å®ç°24hå˜åŒ–è®¡ç®—
            total_change_7d=0.0,   # TODO: å®ç°7då˜åŒ–è®¡ç®—
            total_change_30d=0.0,  # TODO: å®ç°30då˜åŒ–è®¡ç®—
            asset_count=len(assets),
            top_performer=top_performer,
            worst_performer=worst_performer,
            diversity_score=diversity_score
        )

    def _calculate_diversity_score(self, asset_values: List[Dict], total_value: float) -> float:
        """è®¡ç®—æŠ•èµ„ç»„åˆå¤šæ ·åŒ–è¯„åˆ†ï¼ˆ0-1ï¼Œ1è¡¨ç¤ºæœ€å¤šæ ·åŒ–ï¼‰"""
        if total_value <= 0 or len(asset_values) <= 1:
            return 0.0
        
        # è®¡ç®—æ¯ä¸ªèµ„äº§çš„å æ¯”
        proportions = [asset['value'] / total_value for asset in asset_values]
        
        # ä½¿ç”¨é¦™å†œç†µè®¡ç®—å¤šæ ·åŒ–ç¨‹åº¦
        import math
        entropy = 0.0
        for p in proportions:
            if p > 0:
                entropy -= p * math.log2(p)
        
        # æ ‡å‡†åŒ–åˆ°0-1èŒƒå›´
        max_entropy = math.log2(len(asset_values))
        return entropy / max_entropy if max_entropy > 0 else 0.0

    async def _save_enhanced_snapshot(
        self, 
        timestamp: int, 
        date: str, 
        assets: List[Any], 
        metrics: PortfolioMetrics,
        collection_type: str
    ):
        """ä¿å­˜å¢å¼ºçš„å¿«ç…§æ•°æ®"""
        try:
            async with db_manager.get_connection() as conn:
                # ä¿å­˜æŠ•èµ„ç»„åˆçº§åˆ«çš„å¿«ç…§
                await conn.execute("""
                    INSERT OR REPLACE INTO portfolio_snapshots 
                    (timestamp, date, total_value, asset_count, 
                     diversity_score, collection_type, metrics_json)
                    VALUES (?, ?, ?, ?, ?, ?, ?)
                """, (
                    timestamp, date, metrics.total_value, metrics.asset_count,
                    metrics.diversity_score, collection_type,
                    self._serialize_metrics(metrics)
                ))
                
                # ä¿å­˜æ¯ä¸ªèµ„äº§çš„è¯¦ç»†å¿«ç…§
                for asset in assets:
                    await conn.execute("""
                        INSERT OR REPLACE INTO asset_snapshots_enhanced
                        (timestamp, date, asset_id, quantity, price_usdc, 
                         value_usdc, collection_type)
                        VALUES (?, ?, ?, ?, ?, ?, ?)
                    """, (
                        timestamp, date, asset.id,
                        getattr(asset, 'quantity', 0) or 0,
                        getattr(asset, 'price_usdc', 0) or 0,
                        getattr(asset, 'value_usdc', 0) or 0,
                        collection_type
                    ))
                
                await conn.commit()
                
        except Exception as e:
            logger.error(f"ä¿å­˜å¢å¼ºå¿«ç…§æ•°æ®å¤±è´¥: {e}")

    def _serialize_metrics(self, metrics: PortfolioMetrics) -> str:
        """åºåˆ—åŒ–æŒ‡æ ‡æ•°æ®ä¸ºJSON"""
        return json.dumps({
            'total_change_24h': metrics.total_change_24h,
            'total_change_7d': metrics.total_change_7d,
            'total_change_30d': metrics.total_change_30d,
            'top_performer': metrics.top_performer,
            'worst_performer': metrics.worst_performer,
        })

    async def _generate_daily_analytics(self):
        """ç”Ÿæˆæ—¥åº¦åˆ†ææŠ¥å‘Š"""
        try:
            logger.info("ğŸ“Š å¼€å§‹ç”Ÿæˆæ—¥åº¦åˆ†ææŠ¥å‘Š")
            
            # è·å–è¿‡å»24å°æ—¶çš„æ•°æ®
            end_time = int(time.time())
            start_time = end_time - 24 * 60 * 60
            
            # åˆ†ææŠ•èµ„ç»„åˆå˜åŒ–è¶‹åŠ¿
            trend_analysis = await self._analyze_portfolio_trends(start_time, end_time)
            
            # ä¿å­˜åˆ†æç»“æœ
            await self._save_analytics_report(trend_analysis)
            
            logger.info("âœ… æ—¥åº¦åˆ†ææŠ¥å‘Šç”Ÿæˆå®Œæˆ")
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆæ—¥åº¦åˆ†ææŠ¥å‘Šå¤±è´¥: {e}")

    async def _analyze_portfolio_trends(self, start_time: int, end_time: int) -> Dict[str, Any]:
        """åˆ†ææŠ•èµ„ç»„åˆè¶‹åŠ¿"""
        # TODO: å®ç°è¯¦ç»†çš„è¶‹åŠ¿åˆ†æé€»è¾‘
        return {
            'period': f"{datetime.fromtimestamp(start_time)} - {datetime.fromtimestamp(end_time)}",
            'analysis_type': 'daily_trend',
            'generated_at': datetime.now().isoformat()
        }

    async def _save_analytics_report(self, analysis: Dict[str, Any]):
        """ä¿å­˜åˆ†ææŠ¥å‘Š"""
        try:
            async with db_manager.get_connection() as conn:
                await conn.execute("""
                    INSERT INTO analytics_reports 
                    (timestamp, report_type, analysis_json)
                    VALUES (?, ?, ?)
                """, (
                    int(time.time()),
                    analysis.get('analysis_type', 'unknown'),
                    json.dumps(analysis)
                ))
                await conn.commit()
        except Exception as e:
            logger.error(f"ä¿å­˜åˆ†ææŠ¥å‘Šå¤±è´¥: {e}")

    async def get_enhanced_trend_data(
        self, 
        time_range: str = '30d',
        collection_type: str = 'hourly'
    ) -> List[TrendPoint]:
        """è·å–å¢å¼ºçš„è¶‹åŠ¿æ•°æ®"""
        try:
            # è®¡ç®—æ—¶é—´èŒƒå›´
            end_time = int(time.time())
            if time_range == '1d':
                start_time = end_time - 24 * 60 * 60
            elif time_range == '7d':
                start_time = end_time - 7 * 24 * 60 * 60
            elif time_range == '30d':
                start_time = end_time - 30 * 24 * 60 * 60
            elif time_range == '90d':
                start_time = end_time - 90 * 24 * 60 * 60
            else:
                start_time = 0  # å…¨éƒ¨æ•°æ®
            
            async with db_manager.get_connection() as conn:
                cursor = await conn.execute("""
                    SELECT timestamp, date, total_value, asset_count, 
                           diversity_score, metrics_json
                    FROM portfolio_snapshots 
                    WHERE timestamp >= ? AND collection_type = ?
                    ORDER BY timestamp ASC
                """, (start_time, collection_type))
                
                rows = await cursor.fetchall()
                
                trend_points = []
                for row in rows:
                    # è§£ææŒ‡æ ‡æ•°æ®
                    metrics = json.loads(row[5]) if row[5] else {}
                    
                    trend_point = TrendPoint(
                        timestamp=row[0],
                        date=row[1],
                        total_value=row[2],
                        asset_count=row[3],
                        top_assets=[],  # TODO: ä»è¯¦ç»†å¿«ç…§ä¸­è·å–
                        price_changes=metrics.get('price_changes', {}),
                        portfolio_distribution={}  # TODO: è®¡ç®—åˆ†å¸ƒ
                    )
                    trend_points.append(trend_point)
                
                return trend_points
                
        except Exception as e:
            logger.error(f"è·å–å¢å¼ºè¶‹åŠ¿æ•°æ®å¤±è´¥: {e}")
            return []

    async def initialize_enhanced_tables(self):
        """åˆå§‹åŒ–å¢å¼ºå†å²æ•°æ®è¡¨"""
        try:
            async with db_manager.get_connection() as conn:
                # æŠ•èµ„ç»„åˆå¿«ç…§è¡¨
                await conn.execute("""
                    CREATE TABLE IF NOT EXISTS portfolio_snapshots (
                        id INTEGER PRIMARY KEY AUTOINCREMENT,
                        timestamp INTEGER NOT NULL,
                        date TEXT NOT NULL,
                        total_value REAL NOT NULL,
                        asset_count INTEGER NOT NULL,
                        diversity_score REAL DEFAULT 0,
                        collection_type TEXT NOT NULL,
                        metrics_json TEXT,
                        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                    )
                """)
                
                # å¢å¼ºçš„èµ„äº§å¿«ç…§è¡¨
                await conn.execute("""
                    CREATE TABLE IF NOT EXISTS asset_snapshots_enhanced (
                        id INTEGER PRIMARY KEY AUTOINCREMENT,
                        timestamp INTEGER NOT NULL,
                        date TEXT NOT NULL,
                        asset_id TEXT NOT NULL,
                        quantity REAL NOT NULL,
                        price_usdc REAL NOT NULL,
                        value_usdc REAL NOT NULL,
                        collection_type TEXT NOT NULL,
                        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                    )
                """)
                
                # åˆ†ææŠ¥å‘Šè¡¨
                await conn.execute("""
                    CREATE TABLE IF NOT EXISTS analytics_reports (
                        id INTEGER PRIMARY KEY AUTOINCREMENT,
                        timestamp INTEGER NOT NULL,
                        report_type TEXT NOT NULL,
                        analysis_json TEXT NOT NULL,
                        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                    )
                """)
                
                # åˆ›å»ºç´¢å¼•
                await conn.execute("""
                    CREATE INDEX IF NOT EXISTS idx_portfolio_snapshots_timestamp 
                    ON portfolio_snapshots(timestamp)
                """)
                
                await conn.execute("""
                    CREATE INDEX IF NOT EXISTS idx_asset_snapshots_enhanced_timestamp 
                    ON asset_snapshots_enhanced(timestamp, asset_id)
                """)
                
                await conn.commit()
                logger.info("âœ… å¢å¼ºå†å²æ•°æ®è¡¨åˆå§‹åŒ–å®Œæˆ")
                
        except Exception as e:
            logger.error(f"åˆå§‹åŒ–å¢å¼ºå†å²æ•°æ®è¡¨å¤±è´¥: {e}") 